# MPS Inference API ðŸ”¥

A multi-mode image classification API running entirely on Apple Silicon (M1/2) via PyTorchâ€™s MPS backendâ€”no cloud GPUs needed!

## Features:

* FP32 inference (`/infer`)
* Dynamic quantized inference (`/infer-quant`)
* Intelligent batching (`/infer-batch`)
* Steady-state latency reporting in JSON
* Extensible: swap ResNet-18 & ViT-B/16 backbones via `MODEL_TYPE`

## ðŸš€ Quickstart

```bash
# 1. Clone repository
git clone https://github.com/JanakiSubu/MPS-Inference-API.git
cd MPS-Inference-API

# 2. Setup environment
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# 3. Choose model type
export MODEL_TYPE=resnet   # or "vit"

# 4. Run server
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

**Test endpoints:**

```bash
# Full-precision
curl -F "file=@test.jpg" http://localhost:8000/infer
# Quantized
curl -F "file=@test.jpg" http://localhost:8000/infer-quant
# Batched
curl -F "files=@test.jpg" http://localhost:8000/infer-batch
```

## ðŸ“ˆ Benchmarks (steady-state)

| Endpoint       | Latency (ms) |              Speedup vs FP32 |
| -------------- | -----------: | ---------------------------: |
| `/infer`       |     **17.0** |                           1Ã— |
| `/infer-quant` |     **13.1** |                     **1.3Ã—** |
| `/infer-batch` |     **44.8** | (4Ã— batch â†’ \~11.2 ms/image) |

## ðŸ”§ Features

* Zero setup: runs out-of-the-box on M1/2 with PyTorch MPS
* Dynamic Quantization: speed vs. accuracy tradeoff
* Async Batching: groups requests via asyncio queue
* Lightweight: <300 lines of code total
* Plug-and-Play: swap backbones with `MODEL_TYPE`

## ðŸ“ Repository Structure

```text
mps-inference-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py       # API endpoints & batching
â”‚   â”œâ”€â”€ model.py      # load & quantize logic
â”‚   â”œâ”€â”€ utils.py      # async MPS preprocessing
â”‚   â””â”€â”€ batcher.py    # queue + worker
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ benchmark.py  # warm-up & steady-state tests
â”‚   â””â”€â”€ map_labels.py # map ImageNet index â†’ label
â”œâ”€â”€ benchmark_chart.png  # bar chart of latencies
â”œâ”€â”€ requirements.txt   # dependencies list
â”œâ”€â”€ README.md          # this file
â””â”€â”€ LICENSE            # MIT License
```

## ðŸ“‚ How it works

1. Upload images via FastAPI `UploadFile` objects
2. Preprocess (resize, normalize) on the MPS device
3. Infer through ResNet-18 or ViT-B/16
4. Return JSON with `predictions` and `latency_ms` fields
5. Batch endpoint uses an asyncio queue + background worker to group requests

## ðŸ› ï¸ Contributing

1. â­ Star the repo
2. ðŸ”€ Fork & create a Pull Request
3. ðŸ› Report issues
4. ðŸš€ Submit enhancements (e.g., explainability, new backbones)

